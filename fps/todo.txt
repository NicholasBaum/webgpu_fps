check  buildAsync on IBufferObject
check how to improve LightSourceRenderer code
current idea binding should just be a definition file 
stuff has to be initialized and attached manually 
for convinience could build a BindGroupEntry type in parallel and retrive it after pipe was built

PbrRenderer constructor should just take the parameters it acutally needs
    create functions could take others
rename renderShadowMap Proptery
name piepline actualPipeline in NewPipeBuilder
create custom class for blinnphong pipes
need to check if texture dummies are mandatory
BufferObjectBase device should be undefined not null

not using device in a constructor is messy because it disallows to use any GPURefObjects in a constructor
and required references to e.g. the shadow map will have to be late bound e.g. render(shadowmap, environmentmap, instances, material)
or one could do render(RequiredResourcesObject){bind}
or every binding is a BindValueRef type e.g. shadowmap would implement a getTextureInterface
this already fails when Material exposes multiple textures
can be fixed with e.g. TexRef(()=>{return mat.ambientTexture})
problem when mat is changing as well then it would become TexRef(()=>{return this.pipe._currentMat.ambientTexture})
too complicated for starters

on render call everything should just be assigned
how to be less redundant?
currentImp is ok, pipe actually holds resources but can also be reset
stuff thats already available on construction can bei assigned

so question remains what about reusing resources by passing them into the constructor
well thats wrong they should be passed in the buildAsync funciton, what seems tedious
but might be ok and clear if wrapped with build functions

as hotswapping e.g. environmentmaps is actually a thing, the environmentmap actually shouldnt be assigned in the constructor
and neither in buildAsync
so it acutally all has to be forwared into the render call
wrapping stuff might be a good ideas
actually there should be a BindGroupEntrysBuilder taking the pipeline when building

=> split LayoutBuilder and EntryBuilder

rewrite Lights to use BufferObject
rewrite camera to use BufferObject
use api for PBR/Blinn pipelines      

buildAsync on Bindings got questionable as no assigend entries is actually not to uncommon
vbo and NewPipeBuilder isn't correct, either have some like VertexBufferBinding that can be updated by a VertexBufferObject or remove it completley
need to validate in NewPipeBuilder that replaceVertexBuffer is valid buffer
also adding after building is ugly perhaps some pipe is dirty featureor actually split builder and pipe?
Light.diffuse color et al. is set to 0.5, should probably set to 1 but need to check how it influences blinn and pbr shading

load 3d models
resizable canvas
try to copy these images https://www.rombo.tools/2021/11/26/interfaced-lambertians/
3d scan fancy tea saucers and render

deffered rendering
area light
raytraced shadows
cleanup architecture too man piepline builder files...
implement screenspace reflection
implement screenspace gi
implement cryengine lightning
clear coat, fabric, multiscatter shader see https://google.github.io/filament/Filament.html
   

https://nicholasbaum.github.io/webgpu_fps/



Resources / Texture      
    - no tracking of already loaded textures
        - i think this can be solved by a static AssetLoader internally using a map with e.g. key type <device, filepath[]> 
            potentially multiple filepaths e.g. if 6 png's are loaded as cubemap                 
    - GPUTexture/Views could be unloaded when used
        - currently not a problem and no need to solve it ahead of time
    - could intercept GPUTexture.createView(...) calls
        see https://github.com/greggman/webgpu-dev-extension/blob/main/extension/scripts/add-descriptors.js
        i think this wouldn't solve the problem of missing properties in GpuTextureViews when the discriptor was undefined in the createView call

Ui
    - write more flexible UI e.g. like https://github.com/ocornut/imgui    
    - in js https://github.com/greggman/ImHUI
    - example https://greggman.github.io/ImHUI/


Done
    Tonemapping
    Hdr Rendering
    IBL lightning
    Irradiance Map Builder
    Target Light
    Shadow Maps
    UV Tiling
    Normal Mapping
    Ui
    Multiple Lights
    Blinn-Phong Shader
    Implemented Texturing with MipMaps
    Instance Rendering
    Box and Pipe Geometry with Normals
    
    
    
Remark
    -   implement cascaded shadowmaps and/or variance shadow map
        ideas for better shadow maps: choose bias by angle, use backface culling, better light view determination
        acutally simply offsetting the vertices by its normal worked, atm it's a constant offset value,
        probably should depend on scene size, shadow map size in view or worlspace...
    -   merging shaders by string concatenation, could be fixed by some more sophisticated string substitution module (preprocessor)
        with some error message interception to fix the wrong line numbers (see https://jsgist.org/?src=cb4acc6a854a7176e88af7e6a145130d)
        only the last group of a shader can be left unset what isn't really helpful
    -   normal map rendering and ambient don't work to well together as when you have no light only the ambient is rendered
        but no normal map effect is visible this happens in shadow map cases and when surface isn't facing a light
    -   as far as i understood fragment shaders always output rgba and if the target has bgra8unorm format it will be converted automatically

Backlog  
    -   handle TODO in code base    
    -   target light needs a sophisticated near/far plane value determination
    -   Depthmap viewer with sliders for near far plane values in texture_renderer.wgsl, alternatively deduce good values from scene
    -   Omni/Point Light Shadow includes using a cubemap
    -   Shadows from NormalMap
    -   Shadow Voulumes
    -   Raytraced Shadows
    -   renderer/pipeline builder could compose BindGroup instead of GPUBuffers, meaning types lile Material hold BindGroups instead of buffer,
        but can't see much of an advantage at the moment    
    -   create smoothed tangent vectors like in the rust webgpu normalmap example
    -   parralax mapping
        https://webgpu.github.io/webgpu-samples/samples/normalMap#./normalMap.wgsl
    -   point lights can illuminate backfacing faces if normal maps are used, in some cases this is actually correct in some it isn'tangent
        shadow maps actually fix this but are missing in case of omnilights
    -   ambient light and normal maps don't work to well together because bump on a rough surface can have an almost 90 degree normal,
        which wil gather ambient light in an almost 180 degree angle by utilizing the precalculated environment maps,
        but this irradiance is actually coming from behind der surface.
    -   performance measuring doesn't seem to work, results seem to be random, probably because of some async gpu work leading to wrong time measurements
        but testing seemed to point to the irradiance map creation task as culprit
        either find another creation method e.g. importance sampling or wait till light probe lightning and spherical harmonics replaces the algorithm
    -   texture viewers should probably only use the textureLoad function not sample anyhting and show textrues in correct resolution and tonemap if rgba16float format
    -   in pbr.wgsl int the line  let diffuse = (irradiance * albedo) / PI; 
        i'm not sure if I need to divide by PI the following link might give some insights
        https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/
        i think it depends on if you irradiance map generator is already dividing by pi
    -   use compute shader for irradiance map
    -   render debug light cube with its lightcolor, should write a shader for this, 
        that dosn't take the color from a texture for this but from a uniform, so i can render all lights in one pass
    -   refactor Material to use the new Texture type 
    -   use texture_2d_array for rendering multiple instances with different textures
    -   ModelFactory mesh creation could use caches for cases where the same parameters are used multiple times
    -   VertexBufferObject always writes to the gpu even if already loaded, if changed take care of changing device
    -   ModelFactory uses static meshes which will fail when using multiple devices at once
    -   Engine loading scene logics are useles as every is recreated anyways either reuse some stuff e.g. canvas or recreate Engine with scene
    -   VertexBufferObject used for NormalData has properties that aren't actually used by the type e.g. topology is already set in the first vertexbuffer
    -   selection of current renderer aka LightView, TextureView and MainRenderer is super ugly
    -   rewrite shadowmap renderer, texture builder with NewPipeBuilder api
