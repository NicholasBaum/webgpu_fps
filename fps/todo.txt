try to rewrite pipeline generation:
goal: simple api to render cube meshes with a color from a uniform array (colored debug lights example)
further goal: use api for TextureRenderer, EnvironmentMapRenderer, TextureBuilder?, ..., PBR/Blinn pipelines
    - check how e.g. a UniformBufferObject could be helpful
    - function taking json object and creating a BindingGroupDefinition from that
    - function creating bindinggroup from same input
    - BindGroupBuilder class doing the above but stateful also to get per frame updates
    - function taking BindGroupDefintion (wrapper?), constants, VertexBufferDefintion and hopefully not more

load 3d models
resizable canvas
try to copy these images https://www.rombo.tools/2021/11/26/interfaced-lambertians/
3d scan fancy tea saucers and render

deffered rendering
area light
raytraced shadows
cleanup architecture too man piepline builder files...
implement screenspace reflection
implement screenspace gi
implement cryengine lightning
clear coat, fabric, multiscatter shader see https://google.github.io/filament/Filament.html
   

https://nicholasbaum.github.io/webgpu_fps/


Done
    Tonemapping
    Hdr Rendering
    IBL lightning
    Irradiance Map Builder
    Target Light
    Shadow Maps
    UV Tiling
    Normal Mapping
    Ui
    Multiple Lights
    Blinn-Phong Shader
    Implemented Texturing with MipMaps
    Instance Rendering
    Box and Pipe Geometry with Normals
    
    
    
Remark
    -   implement cascaded shadowmaps and/or variance shadow map
        ideas for better shadow maps: choose bias by angle, use backface culling, better light view determination
        acutally simply offsetting the vertices by its normal worked, atm it's a constant offset value,
        probably should depend on scene size, shadow map size in view or worlspace...
    -   merging shaders by string concatenation, could be fixed by some more sophisticated string substitution module (preprocessor)
        with some error message interception to fix the wrong line numbers (see https://jsgist.org/?src=cb4acc6a854a7176e88af7e6a145130d)
        only the last group of a shader can be left unset what isn't really helpful
    -   normal map rendering and ambient don't work to well together as when you have no light only the ambient is rendered
        but no normal map effect is visible this happens in shadow map cases and when surface isn't facing a light
    -   as far as i understood fragment shaders always output rgba and if the target has bgra8unorm format it will be converted automatically

Backlog  
    -   handle TODO in code base    
    -   target light needs a sophisticated near/far plane value determination
    -   Depthmap viewer with sliders for near far plane values in texture_renderer.wgsl, alternatively deduce good values from scene
    -   Omni/Point Light Shadow includes using a cubemap
    -   Shadows from NormalMap
    -   Shadow Voulumes
    -   Raytraced Shadows
    -   renderer/pipeline builder could compose BindGroup instead of GPUBuffers, meaning types lile Material hold BindGroups instead of buffer,
        but can't see much of an advantage at the moment    
    -   create smoothed tangent vectors like in the rust webgpu normalmap example
    -   parralax mapping
        https://webgpu.github.io/webgpu-samples/samples/normalMap#./normalMap.wgsl
    -   point lights can illuminate backfacing faces if normal maps are used, in some cases this is actually correct in some it isn'tangent
        shadow maps actually fix this but are missing in case of omnilights
    -   ambient light and normal maps don't work to well together because bump on a rough surface can have an almost 90 degree normal,
        which wil gather ambient light in an almost 180 degree angle by utilizing the precalculated environment maps,
        but this irradiance is actually coming from behind der surface.
    -   performance measuring doesn't seem to work, results seem to be random, probably because of some async gpu work leading to wrong time measurements
        but testing seemed to point to the irradiance map creation task as culprit
        either find another creation method e.g. importance sampling or wait till light probe lightning and spherical harmonics replaces the algorithm
    -   texture viewers should probably only use the textureLoad function not sample anyhting and show textrues in correct resolution and tonemap if rgba16float format
    -   in pbr.wgsl int the line  let diffuse = (irradiance * albedo) / PI; 
        i'm not sure if I need to divide by PI the following linkt might give some insights
        https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/
        i think it depends on if you irradiance map generator is already dividing by pi
    -   use compute shader for irradiance map
    -   render debug light cube with its lightcolor, should write a shader for this, 
        that dosn't take the color from a texture for this but from a uniform, so i can render all lights in one pass
    -   refactor Material to use the new Texture type 
    -   use texture_2d_array for rendering multiple instances with different textures
    -   ModelFactory mesh creation could use caches for cases where the same parameters are used multiple times
    -   VertexBufferObject always writes to the gpu even if already loaded, if changed take care of changing device
    -   ModelFactory uses static meshes which will fail when using multiple devices at once
    -   Engine loading scene logics are useles as every is recreated anyways either reuse some stuff e.g. canvas or recreate Engine with scene